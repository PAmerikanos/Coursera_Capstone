{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "<h1>Report</h1>\n<li>Introduction where you discuss the business problem and who would be interested in this project.\n<li>Data where you describe the data that will be used to solve the problem and the source of the data.\n<li>Methodology section which represents the main component of the report where you discuss and describe any exploratory data analysis that you did, any inferential statistical testing that you performed, and what machine learnings were used and why.\n<li>Results section where you discuss the results.\n<li>Discussion section where you discuss any observations you noted and any recommendations you can make based on the results.\n<li>Conclusion section where you conclude the report.", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "The idea of this project will be based on the second provided idea: location recommendation for a restaurant.<br>\nMore specifically, we will recommend locations (in Athens, Greece) for overnight foodcarts/trucks/etc. to service customers in the after hours.<br>\nOur audience would be foodcart/truck owners or fast-food managers looking for the best locations to open or setup their store.<br>\nFoodcarts could take advantage of their mobility and move to other busier locations during the day, derived from modified queries using the same algorithm.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "The needed data will be queried from Foursquare making use of the provided API. Specifically, the data to be requested will concern venue locations (Athens, Greece), categories (bar, clubs, non-food venues), hours (after midnight) and trends (busier venues will be preferred).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Fetching package metadata .............\nSolving package specifications: .\n\n# All requested packages already installed.\n# packages in environment at /opt/conda/envs/DSX-Python35:\n#\ngeopy                     1.17.0                     py_0    conda-forge\nFetching package metadata .............\nSolving package specifications: .\n\n# All requested packages already installed.\n# packages in environment at /opt/conda/envs/DSX-Python35:\n#\nfolium                    0.5.0                      py_0    conda-forge\nLibraries imported.\n"
                }
            ], 
            "source": "!conda install -c conda-forge geopy --yes\nfrom geopy.geocoders import Nominatim\n!conda install -c conda-forge folium=0.5.0 --yes\nimport folium\nprint('Libraries imported.')"
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Libraries imported.\n"
                }
            ], 
            "source": "import numpy as np\nimport pandas as pd\nimport requests # library to handle requests\nfrom pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\nprint('Libraries imported.')"
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/geopy/geocoders/osm.py:143: UserWarning: Using Nominatim with the default \"geopy/1.17.0\" `user_agent` is strongly discouraged, as it violates Nominatim's ToS https://operations.osmfoundation.org/policies/nominatim/ and may possibly cause 403 and 429 HTTP errors. Please specify a custom `user_agent` with `Nominatim(user_agent=\"my-application\")` or by overriding the default `user_agent`: `geopy.geocoders.options.default_user_agent = \"my-application\"`. In geopy 2.0 this will become an exception.\n  UserWarning\n"
                }, 
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "The geograpical coordinate of Athens, Greece are 37.976386, 23.726139.\n"
                }
            ], 
            "source": "address = 'Athens, Greece'\n\ngeolocator = Nominatim()\nlocation = geolocator.geocode(address)\nlatitude = 37.976386  #location.latitude\nlongitude = 23.726139 #location.longitude\nprint('The geograpical coordinate of Athens, Greece are {}, {}.'.format(latitude, longitude))"
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Your credentails:\nCLIENT_ID: 3DO3N550RX4XMAL3WG2TOTCILJBUYE2DDOMIKA410AP4KRKC\nCLIENT_SECRET:YNDFTPZXCG3NGCT5MUGRCEMPKBPZMPZPY2LYWOSQ4YLOEBOB\n"
                }
            ], 
            "source": "CLIENT_ID = '3DO3N550RX4XMAL3WG2TOTCILJBUYE2DDOMIKA410AP4KRKC' # your Foursquare ID\nCLIENT_SECRET = 'YNDFTPZXCG3NGCT5MUGRCEMPKBPZMPZPY2LYWOSQ4YLOEBOB' # your Foursquare Secret\nVERSION = '20180605' # Foursquare API version\n\nprint('Your credentails:')\nprint('CLIENT_ID: ' + CLIENT_ID)\nprint('CLIENT_SECRET:' + CLIENT_SECRET)"
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# function that extracts the category of the venue\ndef get_category_type(row):\n    try:\n        categories_list = row['categories']\n    except:\n        categories_list = row['venue.categories']\n        \n    if len(categories_list) == 0:\n        return None\n    else:\n        return categories_list[0]['name']"
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 6, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "{'meta': {'code': 429,\n  'errorDetail': 'Quota exceeded',\n  'errorType': 'quota_exceeded',\n  'requestId': '5bd03172db04f55c3f02339b'},\n 'response': {}}"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "LIMIT = 100\nradius = 50000\nsection = 'sights'\nquery = 'coffee'\nurl = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(CLIENT_ID, CLIENT_SECRET, VERSION, latitude, longitude, radius, LIMIT)\nurl_id = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&section={}'.format(CLIENT_ID, CLIENT_SECRET, VERSION, latitude, longitude, radius, LIMIT, section)\nurl_cat = 'https://api.foursquare.com/v2/venues/categories?&client_id={}&client_secret={}&v={}'.format(CLIENT_ID, CLIENT_SECRET, VERSION)\nurl_mu = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&query={}'.format(CLIENT_ID, CLIENT_SECRET, VERSION, latitude, longitude, radius, LIMIT, query)\n\nresults = requests.get(url_mu).json()\nresults"
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "ename": "KeyError", 
                    "evalue": "'groups'", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-7-35bd5a52bc88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvenues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'response'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'groups'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'items'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnearby_venues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvenues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# flatten JSON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# filter columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mKeyError\u001b[0m: 'groups'"
                    ], 
                    "output_type": "error"
                }
            ], 
            "source": "venues = results['response']['groups'][0]['items']\n    \nnearby_venues = json_normalize(venues) # flatten JSON\n\n# filter columns\nfiltered_columns = ['venue.name', 'venue.categories', 'venue.location.lat', 'venue.location.lng', 'venue.id']\nnearby_venues = nearby_venues.loc[:, filtered_columns]\n\n# filter the category for each row\nnearby_venues['venue.categories'] = nearby_venues.apply(get_category_type, axis=1)\n\n# clean columns\nnearby_venues.columns = [col.split(\".\")[-1] for col in nearby_venues.columns]\n\nnp.unique(nearby_venues.categories.values)\n#nearby_venues"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#approvedCats = ['Art Gallery', 'Art Museum', 'Bar', 'Bistro', 'Caf\u00e9', 'Cocktail Bar', 'Coffee Shop', \n#                'Event Space', 'Gastropub', 'Hotel', 'Hookah Bar', 'Historic Site', 'Hotel Bar', \n#                'Indie Movie Theater', 'Irish Pub', 'Lounge', 'Movie Theater', 'Museum', \n#                'Music Venue', 'Nightclub', 'Ouzeri', 'Plaza', 'Pub', 'Park', 'Other Nightlife', \n#                'Performing Arts Venue', 'Roof Deck', 'Theater', 'Whisky Bar', 'Wine Bar']\napprovedCats = ['Historic Site', 'History Museum', 'Art Museum', 'Monument / Landmark' , 'Museum']\napproved_venues = nearby_venues[nearby_venues['categories'].isin(approvedCats)]\n#np.unique(approved_venues.categories.values)\napproved_venues.reset_index(inplace=True)\n\n#pd.get_dummies(approved_venues.categories)#.value_counts())"
        }, 
        {
            "source": "<H1>Scrape world capitals coordinates", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Load website source code and retain only code containing table data\nwebsite_url = requests.get('https://www.jasom.net/list-of-capital-cities-with-latitude-and-longitude').text\nwebsite_table = website_url.split('<pre>')\nwebsite_table = website_table[1].split('</pre>')\n# Split into rows\nwebsite_table = website_table[0].split(\"\\n\")\n\n# Split rows into columns\nsplitStr = \"</td>\\\\n<td>\"\nwebsite_list = []\nfor row in website_table:\n    row_df = row.split(\",\")\n    website_list.append(row_df)\n\n# Place cells into dataframe, clear strings from HTML tags, drop NaN cells\nwebsite_df = pd.DataFrame(website_list, columns=['Country', 'Capital', 'Latitude', 'Longitude'])\nwebsite_df.drop([0,1], inplace=True)\nwebsite_df.reset_index(drop=True, inplace=True)\nwebsite_df[\"Capital\"] = website_df[\"Country\"] + \", \" + website_df[\"Capital\"]\nwebsite_df.drop(columns=['Country'], inplace=True)\nwebsite_df.head()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def getSites(names, latitudes, longitudes, query):\n    \n    venues_list=[]\n    LIMIT = 100\n    radius = 5000\n\n    for name, lat, lng in zip(names, latitudes, longitudes):\n        try:\n            print(name)\n        \n            # create the API request URL\n            urlmu = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&query={}'.format(CLIENT_ID, CLIENT_SECRET, VERSION, lat, lng, radius, LIMIT, query)\n            \n            # make the GET request\n            results = requests.get(urlmu).json()[\"response\"]['groups'][0]['items']\n\n            # return only relevant information for each nearby venue\n            venues_list.append([(\n                name, \n                lat, \n                lng, \n                v['venue']['name'], \n                v['venue']['location']['lat'], \n                v['venue']['location']['lng'],  \n                v['venue']['categories'][0]['name']) for v in results])\n        except:\n            break\n\n    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n    nearby_venues.columns = ['Capital', \n                  'CptLat', \n                  'CptLon', \n                  'Shop', \n                  'ShpLat', \n                  'ShpLon', \n                  'ShpCat']\n    \n    return(nearby_venues)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "capital_cof = getSites(names=website_df['Capital'],#.head(5),\n                                latitudes=website_df['Latitude'],\n                                longitudes=website_df['Longitude'],\n                                query='coffee')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "capital_cof\n#np.unique(capital_cof['Site Category'].values)\ncapital_sum_cof = capital_cof.groupby(['Capital']).count()\ncapital_sum_cof.drop(columns=['CptLat','CptLon','Shop','ShpLat','ShpLon'], inplace=True)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "capital_tea = getSites(names=website_df['Capital'],#.head(5),\n                                latitudes=website_df['Latitude'],\n                                longitudes=website_df['Longitude'],\n                                query='tea')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "capital_tea\n#np.unique(capital_tea['Site Category'].values)\ncapital_sum_tea = capital_tea.groupby(['Capital']).count()\ncapital_sum_tea.drop(columns=['CptLat','CptLon','Shop','ShpLat','ShpLon'], inplace=True)"
        }, 
        {
            "source": "capital_cof_sum = pd.get_dummies(capital_cof['Site Category'])\ncapital_cof_sum['Capital'] = capital_cof['Capital']\n\n# move neighborhood column to the first column\nfixed_columns = [capital_cof_sum.columns[-1]] + list(capital_cof_sum.columns[:-1])\ncapital_cof_sum = capital_cof.groupby('Capital').sum()#.reset_index()\ncapital_cof_sum = capital_cof_sum.sort_values(by=['Historic Site'], ascending=False, inplace=True)\ncapital_cof_sum", 
            "cell_type": "raw", 
            "metadata": {}
        }, 
        {
            "source": "capital_tea_sum = pd.get_dummies(capital_tea['Site Category'])\ncapital_tea_sum['Capital'] = capital_tea['Capital']\n\n# move neighborhood column to the first column\nfixed_columns = [capital_tea_sum.columns[-1]] + list(capital_tea_sum.columns[:-1])\ncapital_tea_sum = capital_tea_sum[fixed_columns].groupby('Capital').sum().reset_index()\ncapital_tea_sum = capital_tea_sum.sort_values(by=['Historic Site'], ascending=False, inplace=True)\ncapital_tea_sum", 
            "cell_type": "raw", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "capital_sum = capital_sum_cof.join(capital_sum_tea, how='inner', lsuffix='_coffee', rsuffix='_tea')\ncapital_sum['RatioCT'] = capital_sum['ShpCat_coffee']/capital_sum['ShpCat_tea']\ncapital_sum"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}